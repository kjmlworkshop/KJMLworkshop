[{"Name":"Hyunjung (Helen) Shin","Affiliation":"Ajou University","Title":"Graph-based Semi-Supervised Learning for Genome, Diseasome, and Drugome","Abstract":"In this talk, we present several applications of graph-based machine learning algorithms to networks of genome, diseasome and drugome. In the genome application, we introduce Gene Ranker which produces scores for genes. It employs graph-based semi-supervised algorithm. On the genome network that is built with protein interaction data and WGCNA data of immune patients, it provides ranks for key genes of immune diseases. In case of diseasome application, we present an approach for disease co-occurrence scoring based on gene-disease-symptom network. For this, the algorithm for hierarchical structure of networks is proposed. The similarity matrix of hierarchical structure is huge, sparse, and tri-diagonal. To make matters worse, the sub-matrices are not necessarily square. Therefore, we propose an algorithm not only alleviates the problem of non-squareness and sparseness but also solves scalability problem.","Pic":"hshin.jpg"},{"Name":"Yung-Kyun Noh","Affiliation":"Seoul National University","Title":"Inference and Estimation using Nearest Neighbors","Abstract":"","Pic":"placeholder.jpg"},{"Name":"Frank C. Park","Affiliation":"Seoul National University","Title":"Riemannian geometry and machine learning for non-Euclidean data","Abstract":"A growing number of problems in machine learning involve data that is non-Euclidean. A naive application of existing learning algorithms to such data often produces results that depend on the choice of local coordinates used to parametrize the data. At the same time, many problems in machine learning eventually reduce to an optimization problem, in which the objective is to find a mapping from one curved space into another that best preserves distances and angles. We show that these and other problems can be naturally formulated as the minimization of a coordinate-invariant functional that measures the proximity to an isometry of a mapping between two Riemannian manifolds. We first show how to construct general coordinate-invariant functionals of mappings between Riemannian manifolds, and propose a family of functionals that measures how close a mapping is to being an isometry. We then formulate coordinate-invariant distortion measures for manifold learning of non-Euclidean data, and derive gradient-based optimization algorithms that accompany these measures. We also address the problem of autoencoder training for non-Euclidean data using our Riemannian geometric perspective. Both manifold learning and autoencoder case studies involving non-Euclidean datasets illustrate both the underlying geometric intuition and advantages of a Riemannian distortion minimization framework.","Pic":"fcp.jpg"},{"Name":"Sun Kim","Affiliation":"Seoul National University","Title":"Modeling (cancer) cells using multi-omics data","Abstract":"Modeling (cancer) cells using multi-omics data is the ultimate research goal in my lab and we have been making a slow but steady progress over a decade for this goal. In this talk, I will present three of the recently submitted (unpublished) manuscripts towards this goal. (1) sequence level:  Ranked k-spectrum kernel for comparative and evolutionary comparison of exons, introns, and CpG islands, (2) transcript level: Cancer subtype classification and modeling by pathway attention and propagation, and (3) epigenetic level: PRISM: Methylation Pattern-based, Reference-free Inference of Subclonal Makeup.<br>\nThe ranked string kernel paper proposes a new string kernel for comparative and evolutionary genomics. Existing string kernel methods have limitations for comparative and evolutionary studies due to the sensitiveness to over represented \\(k\\)-mers when applied on a genome scale. With this bias of over-representations, comparing multiple genomes simultaneously is even more challenging. To address these issues, we propose a novel ranked \\(k\\)-spectrum string (RKSS) kernel First, our RKSS kernel utilizes a common \\(k\\)-mer set across species, or landmarks, that can be used for comparing arbitrary number of genomes. Second, using landmarks, we can use ranks of \\(k\\)-mers rather than \\(k\\)$-mer frequencies that can produce more reliable distances between genomes. Specifically, RKSS kernel is robust when the \\(k\\)-mer pattern is highly biased from repetitive elements or copy number variations as shown in our experiment. To demonstrate the power of RKSS kernel for comparative and evolutionary sequence comparison, we conducted two experiments using 10 mammalian species.<br>Two remaining manuscripts will be presented at this meeting as posters by the first authors. So, I will explain the research problems and the core concepts of our approaches briefly. The cancer pathway attention and propagation paper is to model cancer cells using an ensemble of several hundred deep learning pathway models. Two main ideas for this modeling are: when combining hundred pathway models, we need to catch context-dependent mechanisms of highlighting which pathways are important, thus attention mechanisms; to achieve explainable cancer cell modeling, we used pathway network propagation. The last paper is about PRISM, a computational method to infer cancer sublclones from DNA methylation data.  Each of unknown clone populations are mixed and represented in methylation data and the goal is to decompose the subclonal populations. The main computational challenges are that the number of dimensions is huge, several hundred million, and the data is error prone. We address this daunting problem by utilizing the characteristics of DNA methylation data. First, errors are corrected by modeling methylation patterns of DNMT1 enzyme using HMM. Then, with error-corrected methylation patterns, PRISM focuses on small individual genomic regions, each of which represent the abundance of a subclone. A set of statistics collected from each genomic region is modeled with a beta-binomial mixture. Fitting the mixture with expectation-maximization algorithm finally provides inferred composition of subclones.","Pic":"skim.png"},{"Name":"Bohyung Han","Affiliation":"Seoul National University","Title":"Learning for Single-Shot Confidence Calibration in Deep Neural Networks through Stochastic Inferences ","Abstract":"I present a generic framework to calibrate accuracy and confidence (score) of a prediction through stochastic inferences in deep neural networks. Our algorithm is motivated by the fact that accuracy and score of a prediction are highly correlated with the variance of multiple stochastic inferences given by stochastic depth or dropout. we design a novel variance-weighted confidence-integrated loss function that is composed of the standard cross-entropy loss and KL-divergence from the uniform distribution, where the two terms are balanced based on the variance of stochastic prediction scores. The proposed algorithm presents outstanding confidence calibration performance and improved classification accuracy with two popular stochastic regularization techniques\u2014stochastic depth and dropout\u2014in multiple models and datasets; it alleviates overconfidence issue in deep neural networks significantly by training the networks to achieve prediction accuracy proportional to the confidence of the prediction.","Pic":"bohyung_han.png"},{"Name":"Ha Quang Minh","Affiliation":"RIKEN-AIP","Title":"Covariance matrices and covariance operators: Theory and Applications","Abstract":"Symmetric positive definite (SPD) matrices, in particular covariance matrices, play important roles in many areas of mathematics and  statistics,with numerous applications various different fields,  including machine learning, brain imaging, and computer vision.\r\nThe set of SPD matrices is not a subspace of Euclidean space and consequently algorithms utilizing  the Euclidean metric tend to be suboptimal in practice. A lot of recent research has therefore focused on exploiting the intrinsic geometrical structures of SPD matrices, in particular the view of this set  as a Riemannian manifold. In this talk, we will present a survey of some of the recent developments in the generalization of finite-dimensional covariance matrices to  infinite-dimensional covariance operators via kernel methods, along with the corresponding geometrical structures.\r\nThis direction exploits the power of  kernel methods from machine learning in a geometrical framework,both mathematically and algorithmically.\r\nThe theoretical formulation will be illustrated with applications in computer vision, which demonstrate both the power of  kernel covariance operators as well as of the algorithms based on their intrinsic geometry.","Pic":"placeholder.jpg"},{"Name":"Bahareh Kalantar","Affiliation":"RIKEN-AIP","Title":"CONDITIONING FACTORS DETERMINATION FOR LANDSLIDE SUSCEPTIBILITY MAPPING USING SUPPORT VECTOR MACHINE LEARNING","Abstract":"This study investigates the effectiveness of two sets of landslide conditioning variable(s).  Fourteen landslide conditioning variables were considered in this study where they were duly divided into two sets G1 and G2. Two Support Vector Machine (SVM) classifiers were constructed based on each dataset (SVM-G1 and SVM-G2) in order to determine which set would be more suitable for landslide susceptibility prediction. In total, 160 landslide inventory datasets of the study area were used where 70% was used for SVM training and 30% for testing. The intra-relationships between parameters were explored based on variance inflation factors (VIF), Pearson\u2019s correlation and Cohen\u2019s kappa analysis. Other evaluation metrics are the area under curve (AUC).\r\n","Pic":"kalantar.jpg"},{"Name":"Chao Li","Affiliation":"RIKEN-AIP","Title":"Reshuffled Tensor Decomposition with Exact Recovery of Low-rank Components.","Abstract":"Low-rank tensor decomposition (TD) is a promising approach for analysis and understanding of real-world. Many such analyses require correct recovery of the true components from the observed tensor, but such characteristic is not achieved in many existing TD methods.  To exactly recover the true components, we introduce a general class of tensor decomposition models where the tensor is decomposed as the sum of reshuffled low-rank components. The reshuffling operation generalizes the conventional folding (tensorization) operation, and also provides additional flexibility to recover true components of complex data structures. We develop a simple convex algorithm called Reshuffled-TD, and theoretically prove that and  exact  recovery  is  guaranteed  when  a  type of incoherence measure is upper bounded.  The results  on  image  steganography  show  that  our method obtains the state-of-the-art performance, and demonstrate the effectiveness of our method in practice.","Pic":"chao_li.jpg"}]